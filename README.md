# GNX CLI - AI Agent with Desktop & Mobile Control

![Banner](./imgs/banner.jpeg)

## Essential Project Badges
[![Stars](https://img.shields.io/github/stars/gokul6350/GNX-CLI?style=social)](https://github.com/gokul6350/GNX-CLI)
[![Forks](https://img.shields.io/github/forks/gokul6350/GNX-CLI?style=social)](https://github.com/gokul6350/GNX-CLI)
[![Issues](https://img.shields.io/github/issues/gokul6350/GNX-CLI)](https://github.com/gokul6350/GNX-CLI/issues)

## AI Agent Theme Badges
[![GNX-CLI](https://img.shields.io/badge/GNX-CLI-8A2BE2?style=for-the-badge&logo=robot&logoColor=white)]()
[![AI Agent](https://img.shields.io/badge/AI_Agent-Next%20Gen-blueviolet?style=flat&logo=spark&logoColor=yellow)]()
[![Platforms](https://img.shields.io/badge/Desktop%20%26%20Android-E91E63?style=flat&logo=android&logoColor=white)]()

## Status & Tech Badges
[![License](https://img.shields.io/github/license/gokul6350/GNX-CLI)](https://github.com/gokul6350/GNX-CLI/blob/main/LICENSE)
![Python](https://img.shields.io/badge/python-3.10+-blue.svg)
![Status](https://img.shields.io/badge/status-active-success.svg)
[![Discussions](https://img.shields.io/badge/Discussions-007ACC?style=flat&logo=github&logoColor=white)](https://github.com/gokul6350/GNX-CLI/discussions)

**GNX CLI** is a next-generation AI agent capable of perceiving and manipulating real-world interfaces. Built on a modular architecture, it combines **Native Tool Calling** (Llama 4 Scout/Groq) for rapid logic with a specialized **Vision Agent** (Qwen3-VL/Novita) for high-fidelity UI automation on both desktop and mobile. Developed by **Gokulbarath**.

## ğŸ“± Mobile Demo


https://github.com/user-attachments/assets/42a5fde6-f226-480d-ab10-94136493f4ac

This clip shows GNX CLI running a full mobile automation sequence from the latest build.

## ğŸ–¥ï¸ Computer Demo


https://github.com/user-attachments/assets/bec3e8a0-30ce-4096-829c-8e6ca0fb33cd



## ğŸš€ Key Features

- **ğŸ§  Hybrid Intelligence:** Fast orchestrator LLM (Llama 4, Gemini, or GLM) plus specialized VLM (Qwen3-VL) for sight.
- **ğŸ‘ï¸ Autonomous Vision Agent:** Sub-agent loop that can see screens, reason about UI, and act (click, swipe, type).
- **ğŸ”Œ MCP Support:** Works with the Model Context Protocol (GitHub, Filesystem, Memory servers).
- **ğŸ“± Mobile Automation:** Deep ADB integration for taps, swipes, and text input.
- **ğŸ’» Desktop Automation:** Mouse/keyboard control via PyAutoGUI with visual feedback loops.
- **ğŸ“ Modular Tooling:** Atomic tools for file ops, web search, system control, and UI automation.


## ğŸ—ï¸ Architecture

![GNX Architecture](./imgs/architecture.png)
![GNX Sequence Flow](./imgs/sequence_flow.png)




### High-Level Routing

```mermaid
graph TD;
	 User[User Input] --> Engine[GNX Engine];
	 Engine -->|Selects Tool| Router{Tool Router};
    
	 subgraph "Standard Tools"
	 Router -->|File Ops| Files[FileSystem / Search];
	 Router -->|Web| Web[DuckDuckGo / Jina];
	 Router -->|MCP| MCP[MCP Servers];
	 end
    
	 subgraph "Automation & Vision"
	 Router -->|Simple| Atomic[Atomic Actions];
	 Atomic --> Desktop[Desktop Control];
	 Atomic --> Mobile[Mobile/ADB];
    
	 Router -->|Complex UI Tasks| Handoff[activate_vision_agent];
	 Handoff --> VisionLoop((Vision Agent Loop));
	 end
    
	 Files --> Output[Result];
	 Web --> Output;
	 MCP --> Output;
	 Desktop --> Output;
	 Mobile --> Output;
	 VisionLoop --> Output;
    
	 Output --> Engine;
	 Engine --> User;
```

#### Vision Agent Loop
When `activate_vision_agent` is called, the system switches to a VLM-driven feedback loop:

```mermaid
graph TD;
	 Start([Task Received]) --> Capture[Capture High-Res Screenshot];
	 Capture --> VLM[Qwen3-VL Analysis];
    
	 VLM -->|Reasoning + JSON| Decision{Decision};
    
	 Decision -->|Action| Executor[Execute Action];
	 Executor -->|Wait for UI| Capture;
    
	 Decision -->|Terminate| Success([Task Complete]);
	 Decision -->|Error| Fail([Report Failure]);
```

## ğŸ› ï¸ Installation

### Requirements
- Python 3.10+
- Windows/Mac/Linux
- For mobile: ADB (Android Debug Bridge) and a connected Android device

### Setup

```bash
git clone https://github.com/Gokulbarath/GNX-CLI.git
cd "GNX CLI"

python -m venv .venv
.venv\Scripts\activate  # Mac/Linux: source .venv/bin/activate

pip install -r requirements.txt

# Configure environment
copy .env.example .env  # Mac/Linux: cp .env.example .env
```

## ğŸ’» Usage

Start the CLI:

```bash
python main.py
```

### Example Commands

1. General reasoning & files
	- "List all python files in src/tools and tell me what they do."
2. Web search
	- "Search for the latest features in Python 3.13."
3. Vision Agent (mobile)
	- Ensure your Android device is connected via ADB.
	- "Open Settings, find 'Display', and turn on Dark Mode." (Agent navigates, scrolls, and taps based on visual cues.)
4. Vision Agent (desktop)
	- "Open Calculator, calculate 55 * 12, and tell me the result."






## ğŸ“‚ Project Structure

```
GNX CLI/
â”œâ”€â”€ main.py                     # Entry point
â”œâ”€â”€ requirements.txt            # Dependencies
â”œâ”€â”€ README.md                   # This file
â”œâ”€â”€ imgs/                       # Assets (demo, architecture, LAMx)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â””â”€â”€ vision/             # Vision agent loop & prompts
â”‚   â”œâ”€â”€ gnx_engine/             # Orchestrator, adapters, prompts
â”‚   â”œâ”€â”€ mcp/                    # Model Context Protocol client
â”‚   â”œâ”€â”€ tools/
â”‚   â”‚   â”œâ”€â”€ desktop/            # Mouse/keyboard/screenshot
â”‚   â”‚   â”œâ”€â”€ mobile/             # ADB/touch/system
â”‚   â”‚   â”œâ”€â”€ handoff/            # Sub-agent triggers
â”‚   â”‚   â”œâ”€â”€ file_ops.py         # File operations
â”‚   â”‚   â”œâ”€â”€ filesystem.py       # Directory listing
â”‚   â”‚   â”œâ”€â”€ system.py           # System utilities
â”‚   â”‚   â”œâ”€â”€ search.py           # File search
â”‚   â”‚   â”œâ”€â”€ todos.py            # TODO management
â”‚   â”‚   â”œâ”€â”€ web_search.py       # Web search
â”‚   â”‚   â””â”€â”€ ui_automation.py    # UI automation helpers
â”‚   â”œâ”€â”€ ui/                     # Display utilities
â”‚   â”œâ”€â”€ utils/                  # Logging, token counting
â”‚   â””â”€â”€ vision_client/          # VLM API client and types
â””â”€â”€ .env.example                # Environment template
```

## ğŸ§¾ Environment Template

```text
# GNX CLI Environment Variables

# Groq API Key (primary orchestrator)
GROQ_API_KEY=your_groq_api_key_here

# Google Gemini API Key (fallback/alternative)
GOOGLE_API_KEY=your_google_api_key_here

# HuggingFace Token (for V_action vision model)
HF_TOKEN=your_huggingface_token_here

# ZhipuAI API Key (GLM-4.5 text-only series)
ZHIPUAI_API_KEY=your_zhipuai_api_key_here

# Default provider: glm | groq | gemini
GNX_DEFAULT_PROVIDER=glm

# Optional model overrides
# GROQ_MODEL=meta-llama/llama-4-scout-17b-16e-instruct
# GEMINI_MODEL=gemini-1.5-flash
# GLM_MODEL=glm-4.5
```


## ğŸ—ºï¸ Future Roadmap

- [ ] Web UI dashboard
- [ ] Performance optimization and caching
- [ ] Personalization

## ğŸ”— Part of LAMx Project

GNX CLI is a rewritten and evolved version of **[Axolot OS](https://github.com/gokul6350/Axolot-os)**, now optimized as a core component of the **LAMx** projectâ€”an integrated ecosystem for general AI-powered intelligence.

![LAMx Logo](./imgs/LAMx.png)

## ğŸ¤ Contributing

Contributions are welcome! Please open an issue or submit a PR.

## ğŸ“œ License

MIT License â€” see the LICENSE file for details.

---

**Built with â¤ï¸ after a lot of ğŸ’”**
